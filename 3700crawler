#!/usr/bin/env python3

import argparse
from asyncio import sleep
import socket, ssl
from html.parser import HTMLParser

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

SESSION_ID_LEN = 32
CSRF_TOKEN_LEN = 64

class Parser(HTMLParser):
    links_found = ['/'] # paths we have found
    flags_found = []
    recording = 0

    def handle_starttag(self, tag, attrs):
        # a tags provide the next paths to crawl
        if (tag == 'a'):
            for tuple in attrs:
                href = tuple[1]
                # only want to crawl links within this domain
                if href[0] == '/' and href not in Crawler.links_crawled: 
                    self.links_found.append(tuple[1])
        # need csrfmiddlewaretoken for login
        if (tag == 'input'):
            if ('name', 'csrfmiddlewaretoken') in attrs:
                for tuple in attrs:
                    if tuple[0] == 'value':
                        Crawler.csrfmiddlewaretoken = tuple[1]
        if (tag == 'h2'):
                self.recording = 1

    def handle_endtag(self, tag):
        if (tag == "h2"):
            self.recording -= 1

    def handle_data(self, data):
        if self.recording:
            if data.startswith("FLAG"):
                self.flags_found.append(data)
                print(data[6:])
                            
class Crawler:
    sock = None
    parser = None
    csrfmiddlewaretoken = None
    csrftoken = None
    sessionid = None
    next_req = None
    DELIMITER = "\r\n\r\n"

    links_crawled = ["/"] # paths we have crawled
    current_link = ""

    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password


    def recv(self):
        buffer = ''
        while "</html>" not in buffer: 
            data = self.sock.recv(1024).decode('ascii')
            if not data:
                break
            buffer += data
        return buffer


    def send_req(self, request):
        self.sock.send(request.encode('ascii'))
        

    def get_response(self):
        try:
            res = self.recv()
            self.parser.feed(res)
            self.parser.close()
            return res
        except:
           print('could not recv')


    def perform_login(self):
        # first GET /accounts/login/ for cookie data
        try:
            self.send_req(self.create_request('GET', '/accounts/login/'))
            res = self.get_response()
            self.links_crawled.append('/accounts/login/')
            self.set_cookie_data(res)

            try: # then POST /accounts/login/ to log in
                body = "username=%s&password=%s&csrfmiddlewaretoken=%s&next=/fakebook/" % (self.username, self.password, self.csrfmiddlewaretoken)
                self.send_req(self.create_request('POST', '/accounts/login/?next=/fakebook/', True, body))
                return self.get_response()
            except:
                print('POST login error')
        except:
            print('GET login error')


    def set_cookie_data(self, response: str):
        sid_i = response.index("sessionid=") + 10
        self.sessionid = response[sid_i : sid_i + SESSION_ID_LEN]

        tok_i = response.index("csrftoken=") + 10
        self.csrftoken = response[tok_i : tok_i + CSRF_TOKEN_LEN]


    # decide what next request to make based on the previous response
    def response_status(self, response: str):
        next_request = None
        if response != '':
            into_lines = response.split('\r\n')
            if (into_lines[0] == '\n'):
                into_lines.remove('\n')
            first_line = into_lines[0].split(" ")
            status = first_line[1]

            if status[0] == "2": # 2XX Status. Everything OK.
                link = self.pick_link()
                next_request = self.create_request("GET", link)
                self.links_crawled.append(link)
                self.current_link = link

            if status[0] == "3": # 3XX Status. HTTP redirect.
                self.set_cookie_data(response)
                next_request = self.handle_redirects(response)

            if status[0] == "4": # 4XX Status. Forbidden or Not Found.
                print("status is 4XX. Abandon the url.\n")  
                
            if status[0] == "5": # 5XX Status. Service unavailable. Try again.
                next_request = self.create_request("GET", self.current_link)
        
            return next_request

    # pop the next path to crawl from the stack, based on paths weve found and not yet crawled
    def pick_link(self):
        links_to_search = set(self.parser.links_found) - set(self.links_crawled)
        links_to_search.remove("/accounts/logout/")
        if len(links_to_search) == 0:
            print("no more links to search")
        return links_to_search.pop()


    # build request for a 3XX redirection
    def handle_redirects(self, response: str):
        res_to_lines = response.split('\r\n')
        path = res_to_lines[6].split(' ')[1]
        self.current_link = path
        return self.create_request("GET", path)


    # given the type of request (GET or POST), the path within the fakebook domain,
    # and an optional message body, build the request
    def create_request(self, request_type: str, path: str, close = False, body = ""):
        initial = "%s %s HTTP/1.1" % (request_type, path)
        keepalive = "Connection: close" if close else "Connection: keep-alive"
        host = "Host: %s:%s" % (DEFAULT_SERVER, DEFAULT_PORT)
        headers = [initial, keepalive, host]

        if (self.csrftoken is not None) and (self.sessionid is not None):
            cookie = "Cookie: csrftoken=%s; sessionid=%s;" % (self.csrftoken, self.sessionid)
            headers.append(cookie)

        if body != "":
            content_length = "Content-Length: %d" % len(body)
            content_type = "Content-Type: application/x-www-form-urlencoded"
            headers.append(content_length)
            headers.append(content_type)

        return "\r\n".join(headers) + self.DELIMITER + body + self.DELIMITER


    def new_socket(self):
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        contextInstance = ssl.SSLContext() # instantiate socket with tls for HTTPS
        self.sock = contextInstance.wrap_socket(s)
        self.sock.connect((self.server, self.port))


    def run(self):
        self.new_socket()
        self.parser = Parser()

        login_res = self.perform_login()
        self.next_req = self.response_status(login_res)
        self.new_socket()

        connected = True
        more_flags = True

        # crawl until there are more flags to be found
        while more_flags:  
            try:  
                self.send_req(self.next_req)
                res = self.get_response()
                self.next_req = self.response_status(res) 
                if (len(self.parser.flags_found) == 5):
                    more_flags = False
            except socket.error: # upon a send or recv error, create a new socket connection
                print( "reconnecting" )  
                connected = False  
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                contextInstance = ssl.SSLContext()
                self.sock = contextInstance.wrap_socket(s)

                while not connected:
                    try:  
                        self.sock.connect((self.server, self.port))
                        connected = True  
                        print( "re-connection successful" )  
                    except socket.error:  
                        sleep(2)  
        self.sock.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
