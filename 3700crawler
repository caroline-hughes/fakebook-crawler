#!/usr/bin/env python3

import argparse
import socket, ssl
from html.parser import HTMLParser
import xml.etree.ElementTree as ET

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443
USERNAME = "hughes.ca"
PASSWORD = "001081764"


# kosowski.e 001360844

def my_recv(s: socket):
    buffer = ''
    try:
        while "</html>" not in buffer: 
            data = s.recv(1024).decode('ascii')
            if not data:
                break
            buffer += data
    except Exception as loopException:
        print("Exception occurred in loop:", loopException)
    return buffer
    
class Parser(HTMLParser):
    # record of paths weve discovered
    links_found = ['/']
    flags_found = []
    num = 0
    recording = 0

    # attrs is a list of tuples
    def handle_starttag(self, tag, attrs):
        # a tags provide the next paths to crawl
        if (tag == 'a'):
            for tuple in attrs:
                href = tuple[1]
                if href[0] == '/' and href not in Crawler.links_crawled:   # we only care about crawling links in this domain... should be a path within it
                    self.links_found.append(tuple[1])
        # right now, we only care about the input tags for the csrfmiddlewaretoken on login page
        if (tag == 'input'):
            if ('name', 'csrfmiddlewaretoken') in attrs:
                for tuple in attrs:
                    if tuple[0] == 'value':
                        Crawler.csrfmiddlewaretoken = tuple[1]
        if (tag == 'h2'):
                self.recording = 1

    def handle_endtag(self, tag):
        if (tag == "h2"):
            self.recording -= 1

    def handle_data(self, data):
        if self.recording:
            print("Data     :", data)
            if data.startswith("FLAG"):
                self.flags_found.append(data)
                            
class Crawler:
    sock = None
    parser = None
    csrfmiddlewaretoken = None
    csrftoken = None
    sessionid = None

    # record of paths weve crawled (had successful GET request for)
    links_crawled = ['/']
    current_link = ""

    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password

    def send_req(self, request):
        # print("Request to %s:%d was:\n\n" % (self.server, self.port))
        print('\n\nMAKING REQ:\n%s' % request)
        self.sock.send(request.encode('ascii'))
        return self.get_response()
        
    def get_response(self):
        try:

            res = my_recv(self.sock)
            print("RESPONSE:\n%s" % res)
            self.parser.feed(res)
            self.parser.close()
            return res
        except:
            print('could not my_recv')

    def login_POST(self):
        if (self.sessionid is None or self.csrftoken is None or self.csrfmiddlewaretoken is None):
            print('not POSTing, missing necessary login data')

        initial = "POST /accounts/login/?next=/fakebook/ HTTP/1.0"
        body = "username=%s&password=%s&csrfmiddlewaretoken=%s&next=/fakebook/" % (self.username, self.password, self.csrfmiddlewaretoken)

        # headers
        c_length = "Content-Length: %d" % len(body)
        #print('my c_length: ', c_length)
        c_type = "Content-Type: application/x-www-form-urlencoded"
        keepalive = "Connection: close"
        cookie = "Cookie: csrftoken=%s; sessionid=%s;" % (self.csrftoken, self.sessionid)
        #print('my cookie: ', cookie)

        return initial + "\r\n" + cookie + "\r\n" + c_length + "\r\n" + c_type + "\r\n" + keepalive + "\r\n\r\n" + body + "\r\n\r\n"

    def perform_login(self):
        # 1. GET /accounts/login/, for cookie data
        res = self.send_req("GET /accounts/login/ HTTP/1.0\r\nConnection: keep-alive\r\n\r\n")
        self.links_crawled.append('/accounts/login/')


        # TODO: better way than .index
        sid_i = res.index("sessionid=") + 10
        self.sessionid = res[sid_i:sid_i + 32] # len(sessionid) = 32

        tok_i = res.index("csrftoken=")+ 10
        self.csrftoken = res[tok_i:tok_i + 64] # len(sessionid) = 64

        # 2. POST /accounts/login/, for actual login
        res2 = self.send_req(self.login_POST())

        self.responseStatus(res2)

        print("FOUND FLAGS: ", self.parser.flags_found)
        print("AMOUNT OF LINKS CRAWLED: ", len(self.links_crawled))




    def pickLink(self):
        links_to_search = set(self.parser.links_found) - set(self.links_crawled)
        links_to_search.remove("/accounts/logout/")
        if len(links_to_search) == 0:
            print("NO MORE LINKS TO SEARCH")
        return links_to_search.pop()

    def responseStatus(self, response):

        if response:
            into_lines = response.split('\n')
            first_line = into_lines[0].split(" ")
            status = first_line[1]

            print("FOUND FLAGS: ", self.parser.flags_found)

            self.refreshSocket()


            if status[0] == "2":
                print("status is 2XX. Everything is OK.\n")
                link = self.pickLink()
                next_request = self.createRequest("GET", link)
                self.links_crawled.append(link)
                self.current_link = link;
                self.responseStatus(self.send_req(next_request))

            if status[0] == "3":
                print("status is 3XX. HTTP redirect. \n")

                sid_i = response.index("sessionid=") + 10
                self.sessionid = response[sid_i:sid_i + 32]  # len(sessionid) = 32

                tok_i = response.index("csrftoken=") + 10
                self.csrftoken = response[tok_i:tok_i + 64]  # len(sessionid) = 64



                new_request = self.handleRedirects(response)
                self.responseStatus(self.send_req(new_request))


            if status[0] == "4":
                print("status is 4XX. Forbidden or Not Found. Abandon the url.\n")

            if status[0] == "5":
                print("status is 5XX. Service unavailable. Resend request.\n")
                next_request = self.createRequest("GET", self.current_link)
                self.responseStatus(self.send_req(next_request))



    def refreshSocket(self):
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        contextInstance = ssl.SSLContext()
        self.sock = contextInstance.wrap_socket(s)
        self.sock.connect((self.server, self.port))



    def handleRedirects(self, response):

        response_into_lines = response.split('\r\n')
        location_line = response_into_lines[6]
        location = location_line.split(' ')[1]
        self.current_link = location;
        new_request = self.createRequest("GET", location)
        return new_request


    def createRequest(self, request_type, location):

        initial = request_type + " " + location + " " + "HTTP/1.0"
        keepalive = "Connection: close"
        cookie = "Cookie: csrftoken=%s; sessionid=%s;" % (self.csrftoken, self.sessionid)

        return initial + "\r\n" + cookie + "\r\n" + keepalive + "\r\n\r\n"




    def fakebook_GET(self):
        initial = "GET /fakebook/ HTTP/1.0"
        keepalive = "Connection: close"
        cookie = "Cookie: csrftoken=%s; sessionid=%s;" % (self.csrftoken, self.sessionid)

        return  initial + "\r\n" + cookie + "\r\n" + keepalive + "\r\n\r\n"

    def run(self):
        # instantiate socket with tls for HTTPS
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        contextInstance = ssl.SSLContext()
        self.sock = contextInstance.wrap_socket(s)
        self.sock.connect((self.server, self.port))
        self.parser = Parser()

        self.perform_login()

        # print('paths found:')
        # print(Parser.links_found)

        # print('paths crawled:')
        # print(self.links_crawled)

        # try to get /fakebook now that im logged in...
        #print("\n\n\n\n GETTING FB")
        #self.send_req(self.fakebook_GET())

        
        self.sock.close()
       

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
